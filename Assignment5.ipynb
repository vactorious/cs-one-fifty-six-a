{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5\n",
    "\n",
    "Victor Huang\n",
    "\n",
    "November 2, 2020\n",
    "\n",
    "1. C\n",
    "2. D\n",
    "3. C\n",
    "4. E\n",
    "5. D\n",
    "6. E\n",
    "7. A\n",
    "8. D\n",
    "9. A\n",
    "10. E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import math\n",
    "import random\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error from error surface\n",
    "def get_error(weights):\n",
    "    return ((weights[0] * math.exp(weights[1])) - (2 * weights[1] * math.exp(-1.0 * weights[0])))**2\n",
    "\n",
    "def get_gradient_u(weights):\n",
    "    return 2 * (math.exp(weights[1]) + (2 * weights[1] * math.exp(-1.0 * weights[0]))) * ((weights[0] * \\\n",
    "        math.exp(weights[1])) - (2 * weights[1] * math.exp(-1.0 * weights[0])))\n",
    "\n",
    "def get_gradient_v(weights):\n",
    "    return 2 * ((weights[0] * math.exp(weights[1])) - (2 * weights[1] * math.exp(-1.0 * weights[0]))) * \\\n",
    "        ((weights[0] * math.exp(weights[1])) - (2 * math.exp(-1.0 * weights[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Function\n",
    "def do_gd_test():\n",
    "    \n",
    "    # Starting weights and learning rate\n",
    "    w = [1.0, 1.0]\n",
    "    n = 0.1\n",
    "    \n",
    "    error = get_error(w)\n",
    "    iterations = 0\n",
    "    while error > 10**(-14):\n",
    "        grad_u = get_gradient_u(w)\n",
    "        grad_v = get_gradient_v(w)\n",
    "        w[0] -= n * grad_u\n",
    "        w[1] -= n * grad_v\n",
    "        error = get_error(w)\n",
    "        iterations += 1\n",
    "        \n",
    "    return (iterations, w)\n",
    "\n",
    "# Coordinate Descent Function\n",
    "def do_coordinate_descent_test():\n",
    "    \n",
    "    # Starting weights and learning rate\n",
    "    w = [1.0, 1.0]\n",
    "    n = 0.1\n",
    "    \n",
    "    error = get_error(w)\n",
    "    for i in range(15):\n",
    "        grad_u = get_gradient_u(w)\n",
    "        w[0] -= n * grad_u\n",
    "        grad_v = get_gradient_v(w)\n",
    "        w[1] -= n * grad_v\n",
    "        error = get_error(w)\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent:\n",
      "Iterations:  10\n",
      "Weights:  [0.04473629039778207, 0.023958714099141746]\n",
      "\n",
      "Coordinate Descent:\n",
      "Error:  0.13981379199615315\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "iterations, w = do_gd_test()\n",
    "print(\"Gradient Descent:\")\n",
    "print(\"Iterations: \", iterations)\n",
    "print(\"Weights: \", w)\n",
    "\n",
    "error = do_coordinate_descent_test()\n",
    "print(\"\\nCoordinate Descent:\")\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logreg(points):\n",
    "    \n",
    "    # Choosing target function points\n",
    "    p1 = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    p2 = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0])\n",
    "    intercept = p1[1] - (slope * p1[0])\n",
    "    \n",
    "    # Determine actual output based on target function\n",
    "    def actual_output(j, k):\n",
    "        return numpy.sign(k - (slope * j + intercept))\n",
    "    \n",
    "    # Choosing data set\n",
    "    data = []\n",
    "    for i in range(points):\n",
    "        random_x = random.uniform(-1, 1)\n",
    "        random_y = random.uniform(-1, 1)\n",
    "        data.append(([1, random_x, random_y], actual_output(random_x, random_y)))\n",
    "        \n",
    "    # Weight vectors\n",
    "    weights = numpy.array([0.0, 0.0, 0.0])\n",
    "    prev_weights = numpy.array([1.0, 1.0, 1.0])\n",
    "    \n",
    "    n = 0.01\n",
    "    epochs = 0\n",
    "    \n",
    "    # Calculate gradient\n",
    "    def gradient(point, output, weights):\n",
    "        return (-output * point) / (1 + math.exp(output * numpy.dot(weights, point)))\n",
    "    \n",
    "    while(numpy.linalg.norm(weights - prev_weights) >= 0.01):\n",
    "        prev_weights = weights\n",
    "        random.shuffle(data)\n",
    "        for point, output in data:\n",
    "            weights = weights - n * gradient(numpy.array(point), output, weights)\n",
    "        epochs += 1\n",
    "        \n",
    "    def cross_entropy_error(weights):\n",
    "        error = 0\n",
    "        \n",
    "        # Generating points\n",
    "        points = []\n",
    "        for i in range(1000):\n",
    "            random_x = random.uniform(-1, 1)\n",
    "            random_y = random.uniform(-1, 1)\n",
    "            points.append(([1, random_x, random_y], actual_output(random_x, random_y)))\n",
    "        for point, output in points:\n",
    "            error += math.log(1 + math.exp(-output * numpy.dot(weights, point)))\n",
    "        return error / 1000\n",
    "    \n",
    "    return(cross_entropy_error(weights), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logreg Testing Function\n",
    "def do_logreg_test(runs, points):\n",
    "    total_cross_entropy_error = 0\n",
    "    total_epochs = 0\n",
    "    for i in range(runs):\n",
    "        cross_entropy_error, epochs = logreg(points)\n",
    "        total_cross_entropy_error += cross_entropy_error\n",
    "        total_epochs += epochs\n",
    "    print(\"Average Cross Entropy Error: \", total_cross_entropy_error / runs)\n",
    "    print(\"Average Epochs: \", total_epochs / runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Entropy Error:  0.10223265835846652\n",
      "Average Epochs:  338.04\n"
     ]
    }
   ],
   "source": [
    "# Driver Code\n",
    "runs = 100\n",
    "do_logreg_test(runs, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
